{
  "hash": "ee76512827848e9daa95394947d4ea6f",
  "result": {
    "markdown": "---\ntitle: \"Bayes Factors: Bayesianische Hypothesentests\"\ndescription: |\n  Eine Alternative zu Null Hypothesis Significance Testing (NHST).\ndate: \"2022-05-15\"\nauthor:\n  - name: Andrew Ellis\n    url: https://github.com/awellis\n    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, UniversitÃ¤t Bern \n    affiliation-url: https://www.kog.psy.unibe.ch\n    orcid: 0000-0002-2788-936X\nlicense: CC BY\ncitation: true\nbibliography: ../../bibliography.bib\nformat:\n    html:\n        toc: true\n        code-link: true\nexecute: \n  cache: false\ncode-annotations: select\n---\n\n\n\n\n:::{.callout-tip}\n## Wichtig\n\nðŸ‘‰ [Daten (`FancyHat.csv`) herunterladen](../../downloadable_files/FancyHat.csv)\n\nðŸ‘‰ [Jasp installieren]([../../downloadable_files/rdkdata_clean.csv](https://jasp-stats.org/))\n:::\n\n\n\n:::{.callout-tip collapse=\"false\"}\n## Lernziele\n\nIn der heutigen Sitzung:\n\n- Modellevergleiche durch Bayes Factors\n- Bayes Factors mit Jasp berechnen\n:::\n\n\n\n\n## What have we done so far?\n\nWe have briefly looked at how to estimate parameters in a very simple model, and we touched upon how to quantify uncertainty in our parameter estimates.\n\n\n\n## What is Bayesian data analysis?\n\n\n- It is important to distinguish between _parameter estimation_ and _hypothesis\ntesting_.\n- Hypothesis testing means comparing models - thus it is more complicated than estimation.\n\nA model is a set of parameters that we are using to predict the observed data. For example, in the card game, we had several different prior assumptions about the abilities of players A and B. One prior expressed our belief that both players were equally good, whereas another prior expressed the belief that either player A or player was better. These priors, together with our assumptions about the distribution of the data, form a model $\\mathcal{M}$.\n\n\n### Bayesian parameter estimation\n\nIn Bayesian parameter estimation, we focus on one model $\\mathcal{M}$. The inferential goal is the posterior distribution, and we can obtain this by applying Bayes' rule (using MCMC or other methods).\n\n\n$$ \np(\\theta | y) =  p(\\theta) \\cdot \\frac{p(y | \\theta)}{p(y)}\n$$\n\n\nWe can rewrite Bayes rule, including the dependency of the parameters $\\mathbf{\\theta}$ on the model $\\mathcal{M}$):\n\n$$ \np(\\theta | y, \\mathcal{M}) = \\frac{p(y|\\theta, \\mathcal{M}) p(\\theta | \\mathcal{M})}{p(y | \\mathcal{M})}\n$$\n\nwhere $\\mathcal{M}$ refers to a specific model. This model is determined by the prior distribution of the parameters $p(\\theta | \\mathcal{M}$ and the distribution of the data $p(y|\\theta, \\mathcal{M})$. \n\nThe marginal likelihood $p(y | \\mathcal{M})$ now gives the probability of the data, averaged over all possible parameter values under prior in model $\\mathcal{M}$.\n\n\n:::{.callout-important}\nThe marginal likelihood $p(y | \\mathcal{M})$ is usually neglected when looking at a single model, but becomes important when comparing models.\n:::\n\nWriting out the marginal likelihood $p(y | \\mathcal{M})$:\n$$ \np(y | \\mathcal{M}) = \\int{p(y | \\theta, \\mathcal{M}) p(\\theta|\\mathcal{M})d\\theta}\n$$\n\nwe see that this is averaged over all possible values of $\\theta$ that the model will allow.\n\nThe priors on $\\theta$ are important, because they determine the probability of possible values of $\\theta$.\n\nThe model evidence will depend on what kind of predictions a model can make. This gives us a measure of complexity â€“ **a complex model is a model that can make many predictions**.\n\nThe problem with making many predictions is that most of these predictions will turn out to be false. The complexity of a model depends on (among other things):\n\n- the number of parameters (as in frequentist model comparison)\n- the prior distributions of the model's parameters\n\nWhen a parameter priors are broad (uninformative), those parts of the parameter space where the likelihood is high are comparatively assigned low probability (because the probability is spread out over the whole parameter space). Intuitively, if one  hedges one's bets, one has to assign low probability to parameter values that make good predictions.\n\n\nThe following prior makes quite a strong prediction about the value of $\\theta$ - it is centred on $0.5$, and the spread is small. Both small and large values are assigned very little probability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta_grid <- seq(0, 1, length = 101)\nprior1 <- dbeta(theta_grid, shape1 = 200, shape2 = 200)\nplot(theta_grid, prior1, \"l\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](bayesian-statistics-3_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe next prior makes less strong predictions, in the sense that it assigns higher probability to small and large values of $\\theta$ (compared to the previous prior, shown as a dashed line). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior2 <- dbeta(theta_grid, shape1 = 2, shape2 = 2)\nplot(theta_grid, prior1, \"l\", lty=\"dashed\")\nlines(theta_grid, prior2, \"l\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](bayesian-statistics-3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nA model that makes more precise predictions, i.e. a model with a stronger prior, is considered less complex that a model that makes many predictions.\n\n\n\n\n\n### Bayesian model comparison\n\nNow consider that we have two models, $\\mathcal{M1}$ and $\\mathcal{M2}$. We would like to know which model explains the data better. We can use Bayes' rule to calculate the posterior probability of $\\mathcal{M1}$ and $\\mathcal{M2}$  (marginalized over all parameters within the model).\n\n$$ \np(\\mathcal{M}_1 | y) = \\frac{P(y | \\mathcal{M}_1) p(\\mathcal{M}_1)}{p(y)} \n$$\n\nand\n\n$$ \np(\\mathcal{M}_2 | y) = \\frac{P(y | \\mathcal{M}_2) p(\\mathcal{M}_2)}{p(y)} \n$$\n\n\nOne way of comparing the models is by taking the ratio $p(\\mathcal{M}_1 | y) / p(\\mathcal{M}_2 | y)$. \n\n$$ \n\\frac{p(\\mathcal{M}_1 | y) = \\frac{P(y | \\mathcal{M}_1) p(\\mathcal{M}_1)}{p(y)}} {p(\\mathcal{M}_2 | y) = \\frac{P(y | \\mathcal{M}_2) p(\\mathcal{M}_2)}{p(y)}}\n$$\n\n\nThe term $p(y)$ cancels out, giving us:\n\n$$ \n\\frac{p(\\mathcal{M}_1 | y) = P(y | \\mathcal{M}_1) p(\\mathcal{M}_1)} {p(\\mathcal{M}_2 | y) = P(y | \\mathcal{M}_2) p(\\mathcal{M}_2)}\n$$\n\n\nOn the left-hand side, we have the ratio of the posterior probabilities of the two models. On the right-hand side, we have the ratio of the marginal likelihoods of the two models, multiplied by the prior probabilities of each model. The  marginal likelihoods (also know as model evidence) tell how well each model explains the data.\n\n$$\n\\underbrace{\\frac{p(\\mathcal{M}_1 | y)} {p(\\mathcal{M}_2 | y)}}_\\text{Posterior odds} = \\underbrace{\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}}_\\text{Ratio of marginal likelihoods} \\cdot \\underbrace{ \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)}}_\\text{Prior odds}\n$$\n\n\n\n$\\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)}$ are the  **prior odds**, and $\\frac{p(\\mathcal{M}_1 | y)}{p(\\mathcal{M}_2 | y)}$ are the **posterior odds**. These tell us which model we believe to be mode probable a priori and a posteriori.\n\nWe are particularly interested in the ratio of the marginal likelihoods:\n\n$$\n\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}\n$$\n\nThis is the **Bayes factor**, and it can be interpreted as the change from prior odds to posterior odds that is indicated by the data.\n\n\nIf we consider the prior odds to be $1$, i.e. we do not favour one model over another a priori, then we are only interested in the Bayes factor. We write this as:\n\n$$ BF_{12} = \\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}$$\n\n\nHere, $BF_{12}$ indicates the extent to which the data support model $\\mathcal{M}_1$ over model $\\mathcal{M}_2$.\n\n\nAs an example, if we obtain a $BF_{12} = 5$, this mean that the data are 5 times more likely to have occured under model 1 than under model 2. Conversely, if $BF_{12} = 0.2$, then the data are 5 times more likely to have occured under model 2.\n\n\nThe following [interpretations](https://www.statology.org/bayes-factor/) are sometimes used (based on @andraszewiczIntroductionBayesianHypothesis), although it is not really necessary or helpful to classify Bayes factors.\n\n\n\n## Hypothesis testing\n\nWe usually perform model comparisons between a null hypothesis $\\mathcal{H}_0$ and an alternative hypothesis $\\mathcal{H}_1$. The terms \"model\" and \"hypothesis\" are used synonymously. A null hypothesis means that we fix the value of the parameter to a certain value, e.g. $\\theta = 0.5$. The alternative hypothesis means that we do not fix the value of the parameter, e.g. we do not assume that the parameter is $0.5$. \n\n:::{.callout-important}\nIt is important to note that the alternative hypothesis needs to be specified. In other words, the parameter(s) need to given a prior distribution.\n:::\n\n\nIn JASP, we will see Bayes factors reported as either\n\n$$ BF_{10} = \\frac{P(y | \\mathcal{H}_1)}{P(y | \\mathcal{H}_0)}$$\n\nwhich indicates a BF for an undirected alternative $\\mathcal{H}_1$ versus the null, or\n\n\n$$ BF_{+0} = \\frac{P(y | \\mathcal{H}_+)}{P(y | \\mathcal{H}_0)}$$\n\nwhich indicates a BF for a directed alternative $\\mathcal{H}_+$ versus $\\mathcal{H}_0$.\n\nIf we want a BF for the null $\\mathcal{H}_0$, we can simply take the inverse of $BF_{10}$:\n\n$$ BF_{01} = \\frac{1}{BF_{10}}$$\n\n\n\n## Savage-Dickey Density Ratio\n\nIf we are comparing two nested models, i.e. a model with 1 free parameter and null model (in which that parameter is fixed to a certain value) we can use the Savage-Dickey density ratio to calculate the Bayes factor.\n\n\n- Under the null model ($\\mathcal{H}_0$ or $\\mathcal{M}_0$): $\\theta = \\theta_0$\n\n- under the alternative model ($\\mathcal{H}_1$ or $\\mathcal{M}_1$): $\\theta \\neq \\theta_0$\n\nWe need to specify a prior distribution of $\\theta$ under $\\mathcal{H}_1$. If we consider the card game example, e.g. $\\theta \\sim \\text{Beta}(1, 1)$.\n\nThe Savage-Dickey Density Ratio  is a simplified manner of obtaining the Bayes factor - we simply need to consider $\\mathcal{M}_1$, and divide the posterior by the prior at the value $\\theta_0$.\n\nLet's look at an example from  @wagenmakersBayesianHypothesisTesting2010a:\n\n\nYou observe that a person correctly answers 9 out of 10 yes-or-no questions.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_correct <- 9\nn_questions <- 10\n```\n:::\n\n\nWe want to know: How likely is that to occur if the person was randomly guessing ($\\theta=0.5$)?\n\nWe will assume a uniform prior over $\\theta$:\n\n$$\np(\\theta) = Beta(1, 1)\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npd <- tibble(\n    x = seq(0, 1, by = .01),\n    Prior = dbeta(x, 1, 1)\n)\nggplot(pd, aes(x, Prior)) +\n    geom_line(size = 1.5) +\n    coord_cartesian(xlim = 0:1, ylim = c(0, 6), expand = 0.01) +\n    labs(y = \"Density\", x = bquote(theta))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](bayesian-statistics-3_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nThe next step is to update our prior with the likelihood, in order to obtain the posterior distribution of $\\theta$:\n\n$$\np(\\theta|y) = Beta(1 + 9, 1 + 1)\n$$\n\nNow we can evaluate both the prior and posterior at the value $0.5$.\n\n\nPrior:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(dprior <- dbeta(0.5, 1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nPosterior:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(dposterior <- dbeta(0.5, 10, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1074219\n```\n:::\n:::\n\n\nThe Bayes factor $BF_{01}$ is the posterior density divided by the prior density:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBF01 <- dposterior / dprior\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBF01\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1074219\n```\n:::\n:::\n\n\n\nThis is not so easy to interpret, so we calculate BF10 instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(BF10 <- 1/BF01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.309091\n```\n:::\n:::\n\n\nThis means that the data are $9.3$ times more likely to have occurred under the alternative than under the null model. \n\nThe following figure is how Bayes factors are usually visualized in JASP. We plot both the prior and posterior distributions, and the density under both, evaluated at $\\theta_0 = 0.5$.\n\nThe density of $\\theta_0$ is smaller after having taking into account the data, or in other words: the probability of $\\theta_0$ has decreased after observing the data. This allows us to conclude that the data (9 out of 10) are approximately 9 times more likely to have occured under $\\mathcal{M1}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npd <- pd |> \n    mutate(Posterior = dbeta(x, 1 + n_correct, 1 + (n_questions-n_correct)))\n\npdw <- pd |> \n  pivot_longer(names_to = \"Type\", \n               values_to = \"density\", \n               Prior:Posterior)\npdw |> \n  ggplot(aes(x, density, col = Type)) +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\") +\n  geom_line(size = 1.5) +\n  scale_x_continuous(expand = expansion(0.01)) +\n  scale_color_viridis_d(end = 0.8) +\n  labs(y = \"Density\", x = bquote(theta)) +\n  annotate(\"point\", x = c(.5, .5), \n           y = c(pdw$density[pdw$x == .5]),\n           size = 4) +\n  annotate(\"label\",\n    x = c(.5, .5),\n    y = pdw$density[pdw$x == .5],\n    label = round(pdw$density[pdw$x == .5], 3),\n    vjust = -.5\n  )\n```\n\n::: {.cell-output-display}\n![](bayesian-statistics-3_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(pd, x == .5) |>\n  mutate(\n    BF01 = Posterior / Prior,\n    BF10 = 1 / BF01) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 5\n      x Prior Posterior  BF01  BF10\n  <dbl> <dbl>     <dbl> <dbl> <dbl>\n1   0.5     1     0.107 0.107  9.31\n```\n:::\n:::\n\n\n\n## Case studies\n\nYou can generate the following datasets, and then import the `CSV` files into JASP.\n\n\n### Card game\n\nPlayer A wins 6 out of 9 games. \n\n- Do the data provide evidence that player A is better than player B?\n- What is the evidence that they are both equally good?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwins <- 6\ngames <- 9\n\ncardgame <- c(rep(1, wins), rep(0, games-wins)) |> \n  sample(games, replace = FALSE)\n\ncardgame <- tibble(game = 1:9,\n                   winner = if_else(cardgame == 1, \"A\", \"B\"),\n                   indicator = cardgame)\n\ncardgame |> write_csv(\"cardgame.csv\")\n```\n:::\n\n\n\n\n### Exam questions\n\nA student correctly answer 9 questions out of 10. \n\n- What is the evidence that the student was guessing (got lucky)?\n- What is the evidence that the student's ability is greater than $0.5$?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_correct <- 9\nn_questions <- 10\n\nquestions <- c(rep(1, n_correct), rep(0, n_questions-n_correct)) |> \n  sample(n_questions, replace = FALSE)\n\nquestions <- tibble(question = 1:n_questions,\n                   correct = if_else(questions == 1, \"correct\", \"error\"),\n                   indicator = questions)\n\nquestions |> write_csv(\"questions.csv\")\n```\n:::\n\n\n\n### Smart drug t-test\n\nGenerate this dataframe, and export it as `CSV` file. In this example, we want to compare two groups' IQ scores. One of the groups has been given a drug to make them smart, the other is the control group.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart = tibble(IQ = c(101,100,102,104,102,97,105,105,98,101,100,123,105,103,\n                      100,95,102,106,109,102,82,102,100,102,102,101,102,102,\n                      103,103,97,97,103,101,97,104,96,103,124,101,101,100,\n                      101,101,104,100,101),\n               Group = \"SmartDrug\")\n\nplacebo = tibble(IQ = c(99,101,100,101,102,100,97,101,104,101,102,102,100,105,\n                        88,101,100,104,100,100,100,101,102,103,97,101,101,100,\n                        101, 99,101,100,100,101,100,99,101,100,102,99,100,99),\n                 Group = \"Placebo\")\n\nSmartDrug <- bind_rows(smart, placebo)  |>\n    mutate(Group = fct_relevel(as.factor(Group), \"Placebo\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(SmartDrug, file = \"SmartDrug.csv\")\n```\n:::\n",
    "supporting": [
      "bayesian-statistics-3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}