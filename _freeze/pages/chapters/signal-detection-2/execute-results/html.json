{
  "hash": "34707d98ebf170e2173bc2ecabdcc1ca",
  "result": {
    "markdown": "---\ntitle: \"Datenanalyse mit SDT\"\ndescription: |\n  SDT Kennzahlen schÃ¤tzen.\ndate: \"2022-04-24\"\nauthor:\n  - name: Andrew Ellis\n    url: https://github.com/awellis\n    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, UniversitÃ¤t Bern \n    affiliation-url: https://www.kog.psy.unibe.ch\n    orcid: 0000-0002-2788-936X\nlicense: CC BY\ncitation: true\nbibliography: ../../bibliography.bib\nformat:\n    html:\n        toc: true\n        code-link: true\nexecute: \n  cache: false\ncode-annotations: select\n---\n\n\n:::{.callout-tip collapse=\"false\"}\n## Lernziele\n\nIn der heutigen Sitzung lernen wir:\n\nðŸš§\n:::\n\n\n\n# Parameter Recovery\n\nTo get a feel for how the parameters `c` and `d'` relate to observed hit and false alarm rates, we will do the following: we first simulate an observer performing a classification experiment with known parameters, i.e. `c` and `d'` are known to us and used to generate the data. We then attempt to recover `c` and `d'` from the observed hit and false alarm rates.\n\nTo do this, we can define a function that takes `c` and `d'` as input, and then simulates $N$ signal and $N$ noise trials, giving a total of $2\\cdot N$ trials. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.2     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.0\nâœ” ggplot2   3.4.2     âœ” tibble    3.2.1\nâœ” lubridate 1.9.2     âœ” tidyr     1.3.0\nâœ” purrr     1.0.1     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_hits_false_alarms <- function(dp = 1, c = 0, N = 100) {\n  nS <- nN <- N\n\n  pFA <- 1 - pnorm(c + dp/2)\n  pH <- 1 - pnorm(c - dp/2)\n  \n  FA <- rbinom(1, nN, pFA)\n  Hit <- rbinom(1, nS, pH)\n  \n  CR <- nN-FA\n  Miss <- nS-Hit\n\n  tibble(Hit, Miss, FA, CR)\n}\n```\n:::\n\n\nWe first calculate the probability of a hit, `pH`, and a false alarm, `pFA`, as they correspond to the area under the curve to the right of the criterion, under both signal and noise distributions, respectively. \n\n```r\npFA <- 1 - pnorm(c + dp/2)\npH <- 1 - pnorm(c - dp/2)\n```\n\n> We are using the bias `c` to parameterize the distributions here. Alternatively, we could also use the criterion `k`, which would result in \n\n```r\npFA <- 1 - pnorm(k)\npH <- 1 - pnorm(k - dp)\n```\n\nThis has the more intuitive interpretation that `pFA` is simply the area under the noise distribution that lies to the right of the criterion `k`, or: \"given that my criterion is $k$, what is the probability that my response was a false alarm?\". However, `c` is a more interesting quantity for us, because it quantifies the devation from an ideal observer.\n\nWe then generate false alarms and hits as binomially distributed random variables, i.e. number of `yes` responses in `N` trials, given the hit and false alarm rates, respectively.\n\n```r\nFA <- rbinom(1, nN, pFA)\nHit <- rbinom(1, nS, pH)\n```\n\nOnce we have the number of hits and false alarms, we can compute the number of misses and correct rejections, given that we know how many trials were performed in each condition.\n\n```r\nCR <- nN-FA\nMiss <- nS-Hit\n```\n\nNow, we can simulate the behaviour of an observer. An ideal observer would be unbiased, i.e. use a value of $c=0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(89)\nideal_observer <- sim_hits_false_alarms(d = 1, c = 0)\nideal_observer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    Hit  Miss    FA    CR\n  <int> <dbl> <int> <dbl>\n1    61    39    26    74\n```\n:::\n:::\n\n\nOne thing to note is that, even an unbiased,  ideal observer cannot achieve perfect performance given that $d=1$. \n\nWe can compute the observer's accuracy as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nideal_observer |> \n  summarise(accuracy = (Hit + CR)/(Hit + CR + Miss + FA))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 1\n  accuracy\n     <dbl>\n1    0.675\n```\n:::\n:::\n\n<aside>\nThere are of course more elegant ways to compute the accuracy.\n</aside>\n\n:::{.callout-note}\nHow can you make the ideal observer achieve an (almost) perfect performance?\n:::\n\nWe can also simulate the behaviour of an observer that is biased to toward giving `yes` responses, i.e. an observer with a value of $c<0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(89)\nyes_observer <- sim_hits_false_alarms(d = 1, c = -1)\nyes_observer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    Hit  Miss    FA    CR\n  <int> <dbl> <int> <dbl>\n1    92     8    74    26\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer |> \n  summarise(accuracy = (Hit + CR)/(Hit + CR + Miss + FA))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 1\n  accuracy\n     <dbl>\n1     0.59\n```\n:::\n:::\n\n\n:::{.callout-tip}\nHere, it should become clear why accuracy by itself is not that informative. The observer that is biased toward saying `yes` will achieve a very high hit rate, but has to trade this off against a very high false alarm rate. If we just look at accuracy, we might think that the biased observer isn't good at the task, but using SDT we may discover that it is the choice of criterion that is to blame, not the observer's ability!\n:::\n\n## Parameter recovery\nWe can now attempt to recover the _known_ parameters `c` and `d'` from the observed hit and false alarm rates.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer <- yes_observer |>\n    mutate(hit_rate = Hit/(Hit + Miss),\n           fa_rate = FA/(FA + CR))\n\nyes_observer <- yes_observer |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\nyes_observer <- yes_observer |>\n    mutate(dprime = zhr - zfa,\n           k = - zfa,\n           c = -0.5 * (zhr + zfa)) |>\n    mutate(across(c(dprime, c), round, 2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\nâ„¹ In argument: `across(c(dprime, c), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 11\n    Hit  Miss    FA    CR hit_rate fa_rate   zhr   zfa dprime      k     c\n  <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1    92     8    74    26     0.92    0.74  1.41 0.643   0.76 -0.643 -1.02\n```\n:::\n:::\n\n\nFor the biased observer, the valuues we used were $d' = 1$ and $c = -1$. Are we able to recover these? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer |> pull(c, dprime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 0.76 \n-1.02 \n```\n:::\n:::\n\n\n:::{.callout-note}\nWhy is it seemingly difficult to recover theses parameters?\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}