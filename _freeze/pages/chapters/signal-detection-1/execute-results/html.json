{
  "hash": "e4de997f9901cc0a3f00d43ebf6e692b",
  "result": {
    "markdown": "---\ntitle: \"Einführung\"\ndescription: |\n  Signal Detection Theory: Entscheidungen unter Unsicherheit.\ndate: \"2022-04-24\"\nauthor:\n  - name: Andrew Ellis\n    url: https://github.com/awellis\n    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universität Bern \n    affiliation-url: https://www.kog.psy.unibe.ch\n    orcid: 0000-0002-2788-936X\nlicense: CC BY\ncitation: true\nbibliography: ../../bibliography.bib\nformat:\n    html:\n        toc: true\n        code-link: true\nexecute: \n  cache: false\ncode-annotations: select\n---\n\n\n:::{.callout-tip collapse=\"false\"}\n## Lernziele\n\nIn der heutigen Sitzung lernen wir:\n\n- Simple binäre Entscheidungen mit Signal Detection Theory modellieren\n- Konzepte der Signal Detection Theory (SDT) \n- SDT mit R simulieren\n:::\n\n# Entscheidungen\n\nSignal detection theory (SDT) is a statistical theory of decision making that can be used to model the performance of a subject in any task requiring a binary decision. SDT has been applied to many areas of psychology and neuroscience. For a slighlty old, but still very good overview, see @goldNeuralBasisDecision2007a. This paper describes several applications of SDT, in particular to the study of random dot motion stimuli.\n\nSDT can be applied whenever two possible stimulus types must be discriminated. Psychologists first applied the theory in studies of perception, where subjects discriminated between signals (stimuli) and noise (no stimuli). The signal and noise labels remain, but SDT has since been applied in many other areas. Examples (and their corresponding signal and noise stimuli) include recognition memory (old and new items), lie detection (lies and truths), personnel selection (desirable and undesirable applicants), jury decision making (guilty and innocent defendants), medical diagnosis (diseased and well patients), industrial inspection (unacceptable and acceptable items), and information retrieval (relevant and irrelevant information\n\nSDT assumes that the decision-maker has to make a binary choice between two alternatives (e.g., signal or noise) based on limited or uncertain information. The decision is made by comparing the received information to a criterion or threshold. If the information exceeds this threshold, the decision-maker will choose the signal; otherwise, they will choose the noise.\n\nOne of the key contributions of SDT is the distinction between sensitivity and bias in the decision-making process. Sensitivity refers to the ability of the decision-maker to correctly distinguish between two different types of signals, whereas bias refers to the tendency of the decision-maker to favor one alternative over the other. By separating these two components, researchers can analyze how different factors affect decision-making performance.\n\n\n\n@goldNeuralBasisDecision2007a\n\n\n# Modelle\n\n## Elements of a Decision\nThe decisions required for many sensorimotor tasks can be thought of as a form of statistical inference\n\nWhat is the (unknown) state of the world, given the noisy data provided by the sensory systems? \n\nIn a model, we want to make sure that the problem is simple, so we restrict the world to two possible states. These can be thought of as hypotheses.\n\n:::{.callout-important}\n## Übung\n\n\n:::{.callout-tip collapse=\"true\"}\n## Lösung\n:::\n\n:::\n\n\n\n# Signal detection theory\n\n@shadlenDecisionMakingWindow2013 and @wylieUsingSignalDetection2020\n\n@knoblauchSignalDetectionTheory2012\n\n@macmillanDetectionTheoryUser2004a\n\nCalculation of SDT measures @stanislawCalculationSignalDetection1999b\n\nWe consider an experiment in which a person has to classify a stimulus into one of two possible categories:\n\n- new / old\n- left / right\n- yes / no\n\nWe can neglect the underlying task, as the math is the same. In the general case, say we present two stimulus categories `A` and `B`, that vary along some dimension. The task of the subject in our experiment is to perform a binary classification with the response options `A` and `B`. The subject's performance can be summarized in a classification table, with four possible outcomes:\n\n\n|              | Signal         |                        |\n|------------- |--------------- |------------------------|\n| **Response** | A (yes)        | B (no)                 |\n| A (yes)      | Hit            | False alarm (FA)       |\n| B (no)       | Miss           | Correct rejection (CR) |\n\n- **Hit**:  Stimulus is `A`, subject responds `A`\n- Miss: Stimulus is `A`, subject responds `B`\n- **False alarm**: Stimulus is `B`, subject responds `A`\n- Correct rejection: Stimulus is `B`, subject responds `B`\n\n```\nThe hit rate equals the proportion of the signal distribution that exceeds the criterion, whereas the false alarm rate equals the proportion of the noise distribution that exceeds the criterion.\n```\n\n> Given the stimulus, the subject has two response options. Therefore, we consider only the `A` responses when the stimulus is `A` (hits) or `B` (false alarms).\n\n- The SDT model assumes that on each trial $i$, a person's information about a stimulus can be modeled as a random variable $X_i$. \n\n- This is drawn from one of two possible distributions, which (in equal variance SDT) differ only in their location, but not their scale ( we assume that $\\sigma = 1$). \n\n> Example: familiarity. When the subject is shown an image, this evokes a feeling of 'familiarity`. This is a latent _strength variable_.\n\n<aside>\nSDT does not require any particular distributions, but in practise, Gaussians are often chosen.\n</aside>\n\n> Thought experiment: you are a subject in a memory experiment. You were previously shown a number of images, and now you are presented with a mixture of old and new items, and have to say whether you have previously seen the test image.\n\nThis can be formulated as the following statistical problem:\n\n1) You are given a random variable $X$, i.e. a draw from a normal distribution with a known standard deviation. You also know that distribution can have either of two known means, you just don't know which one. The two distributions differ only in their mean, and the difference in means is called `d'`.\n\n2) You are asked to say which distribution that $X$ is most likely to have come from. This is a decision, so you need some sort of decision rule. In this case you can choose a criterion, and compare $X$ to this.\n\n3) You will produce four types of responses: you will either correctly classifiy the presented stimulus, or its internal representation $X$, as either `old` or `new`. You will do this correctly (`hits` / `correct rejections`), or you will produce a missclassification (`false alarms` / `misses`).\n\n4) From your behavioural data, the number of `hits` and `false alarms`, we want to estimate your hit rate and false alarm rate, and then compute your internal (latent) quantities that guided your behaviour.\n\nThe internal signal evoked by old and new items is often shown like this:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNew items produce less familiarity than old items, but the internal representation is noisy.\n\nIn order to classify the presented stimulus, based on the evoked familiarity (decision variable), we need a decision rule:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nA simple rule is to compare the signal with a criterion $k$. If the signal $X > k$, then respond `old` (\"Yes, I have previously seen it\"), otherwise respond `new` (\"No, I haven't seen it before\").\n\n## Signal detection parameters\n\nThe commonly known SDT parameters are $d'$ and and $c$.\n\n- d': distance between distributions\n\n$$ d' = c - \\phi^{-1}(1-p_{H}) = \\phi^{-1}(p_{H}) - \\phi^{-1}(p_{FA}) $$\nwhich can also be written as:\n$$ d' = \\phi^{-1}(P(y = 1 | old)) - \\phi^{-1}(P(y = 1 | new)) $$\n\nThe expression for $d'$\n\n- requires estimating probabilities conditional on the identity of a signal\n- requires taking the difference on a transformed (probit) scale\n- this is equivalent to a contrast between levels of a factor with two levels as the linear predictor for a response in a GLM \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n- k: decision criterion\n$$ k = \\phi^{-1}(1-p_{FA}) = -\\phi^{-1}(p_{FA}) $$\n\nBetter: distance to optimal decision boundary (c, or bias)\n$$ c = -\\frac{1}{2} \\left[\\phi^{-1}(p_{H}) + \\phi^{-1}(p_{FA})\\right] $$\n\n> What we are doing here is __estimating__ the hit rate and false alarm rate from observed hits and false alarms, and then computing d' and c from these estimated probabilities.\n\nWe can also write this the other way round:\n\nWhen the stimulus is `new`, we will produce false alarms with probability:\n\n$$ p_{FA} = P(y = 1 | X = 0) = 1 - \\Phi(k) $$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhen the stimulus is `old`, we will produce hits with probability:\n$$ p_{H} = P(y = 1 | X=1) = 1 - \\Phi(k-d') $$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n- We can write this in one equation:\n\n$$ P(y = 1 | X = x) = 1 - \\Phi(k-d'X) = \\Phi(-k + d'X) $$\nwhere $X$ is an indicator variable, i.e. takes the value `1` for `old` and `0` for `new`. \n\nThis produces the probability of giving an `old` response, given the stimulus. If the stimulus is `old`, this is the probability of a `hit`, if the stimulus is `new`, this is the probability of a `false alarm`.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](signal-detection-1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n# Parameter recovery\n\nTo get a feel for how the parameters `c` and `d'` relate to observed hit and false alarm rates, we will do the following: we first simulate an observer performing a classification experiment with known parameters, i.e. `c` and `d'` are known to us and used to generate the data. We then attempt to recover `c` and `d'` from the observed hit and false alarm rates.\n\nTo do this, we can define a function that takes `c` and `d'` as input, and then simulates $N$ signal and $N$ noise trials, giving a total of $2\\cdot N$ trials. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_sdt <- function(dp = 1, c = 0, N = 100) {\n  nS <- nN <- N\n\n  pFA <- 1 - pnorm(c + dp/2)\n  pH <- 1 - pnorm(c - dp/2)\n  \n  FA <- rbinom(1, nN, pFA)\n  Hit <- rbinom(1, nS, pH)\n  \n  CR <- nN-FA\n  Miss <- nS-Hit\n\n  tibble(Hit, Miss, FA, CR)\n}\n```\n:::\n\n\nWe first calculate the probability of a hit, `pH`, and a false alarm, `pFA`, as they correspond to the area under the curve to the right of the criterion, under both signal and noise distributions, respectively. \n\n```r\npFA <- 1 - pnorm(c + dp/2)\npH <- 1 - pnorm(c - dp/2)\n```\n\n> We are using the bias `c` to parameterize the distributions here. Alternatively, we could also use the criterion `k`, which would result in \n\n```r\npFA <- 1 - pnorm(k)\npH <- 1 - pnorm(k - dp)\n```\n\nThis has the more intuitive interpretation that `pFA` is simply the area under the noise distribution that lies to the right of the criterion `k`, or: \"given that my criterion is $k$, what is the probability that my response was a false alarm?\". However, `c` is a more interesting quantity for us, because it quantifies the devation from an ideal observer.\n\nWe then generate false alarms and hits as binomially distributed random variables, i.e. number of `yes` responses in `N` trials, given the hit and false alarm rates, respectively.\n\n```r\nFA <- rbinom(1, nN, pFA)\nHit <- rbinom(1, nS, pH)\n```\n\nOnce we have the number of hits and false alarms, we can compute the number of misses and correct rejections, given that we know how many trials were performed in each condition.\n\n```r\nCR <- nN-FA\nMiss <- nS-Hit\n```\n\nNow, we can simulate the behaviour of an observer. An ideal observer would be unbiased, i.e. use a value of $c=0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(89)\nideal_observer <- sim_sdt(d = 1, c = 0)\nideal_observer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n    Hit  Miss    FA    CR\n  <int> <dbl> <int> <dbl>\n1    61    39    26    74\n```\n:::\n:::\n\n\nOne thing to note is that, even an unbiased,  ideal observer cannot achieve perfect performance given that $d=1$. \n\nWe can compute the observer's accuracy as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nideal_observer |> \n  summarise(accuracy = (Hit + CR)/(Hit + CR + Miss + FA))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1    0.675\n```\n:::\n:::\n\n<aside>\nThere are of course more elegant ways to compute the accuracy.\n</aside>\n\n:::{.callout-note}\nHow can you make the ideal observer achieve an (almost) perfect performance?\n:::\n\nWe can also simulate the behaviour of an observer that is biased to toward giving `yes` responses, i.e. an observer with a value of $c<0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(89)\nyes_observer <- sim_sdt(d = 1, c = -1)\nyes_observer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n    Hit  Miss    FA    CR\n  <int> <dbl> <int> <dbl>\n1    92     8    74    26\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer |> \n  summarise(accuracy = (Hit + CR)/(Hit + CR + Miss + FA))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1     0.59\n```\n:::\n:::\n\n\n:::{.callout-tip}\nHere, it should become clear why accuracy by itself is not that informative. The observer that is biased toward saying `yes` will achieve a very high hit rate, but has to trade this off against a very high false alarm rate. If we just look at accuracy, we might think that the biased observer isn't good at the task, but using SDT we may discover that it is the choice of criterion that is to blame, not the observer's ability!\n:::\n\n## Parameter recovery\nWe can now attempt to recover the _known_ parameters `c` and `d'` from the observed hit and false alarm rates.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer <- yes_observer |>\n    mutate(hit_rate = Hit/(Hit + Miss),\n           fa_rate = FA/(FA + CR))\n\nyes_observer <- yes_observer |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\nyes_observer <- yes_observer |>\n    mutate(dprime = zhr - zfa,\n           k = - zfa,\n           c = -0.5 * (zhr + zfa)) |>\n    mutate(across(c(dprime, c), round, 2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(c(dprime, c), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 11\n    Hit  Miss    FA    CR hit_rate fa_rate   zhr   zfa dprime      k     c\n  <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1    92     8    74    26     0.92    0.74  1.41 0.643   0.76 -0.643 -1.02\n```\n:::\n:::\n\n\nFor the biased observer, the valuues we used were $d' = 1$ and $c = -1$. Are we able to recover these? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nyes_observer |> pull(c, dprime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 0.76 \n-1.02 \n```\n:::\n:::\n\n\n:::{.callout-note}\nWhy is it seemingly difficult to recover theses parameters?\n:::\n\n# Memory experiment\n\nLet's look at an example (borrowing heavily from this [blog post](https://vuorre.netlify.com/post/2017/10/09/bayesian-estimation-of-signal-detection-theory-models-part-1/)).\n\nThe data are from a recognition memory experiment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You might first need to install the `remotes` package\n# install.packages(\"remotes\")\n# install sdtalt\n# remotes::install_github(\"cran/sdtalt\")\n\nlibrary(sdtalt)\nlibrary(tidyverse)\n\ndata(confcontr)\n\nconfcontr <- as_tibble(confcontr) |> \n  mutate(subno = as_factor(subno),\n         item = isold - 0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconfcontr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,100 × 4\n   subno sayold isold  item\n   <fct>  <dbl> <dbl> <dbl>\n 1 53         1     0  -0.5\n 2 53         1     1   0.5\n 3 53         1     1   0.5\n 4 53         1     1   0.5\n 5 53         1     0  -0.5\n 6 53         1     1   0.5\n 7 53         1     0  -0.5\n 8 53         0     0  -0.5\n 9 53         0     1   0.5\n10 53         0     1   0.5\n# ℹ 3,090 more rows\n```\n:::\n:::\n\n\nFirst we classify each response as hit, miss, correct rejection (cr) or false alarm (fa):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt <- confcontr |> \n  mutate(type = case_when(\n        isold==1 & sayold==1 ~ \"Hit\",\n        isold==1 & sayold==0 ~ \"Miss\",\n        isold==0 & sayold==0 ~ \"CR\",\n        isold==0 & sayold==1 ~ \"FA\"))\n```\n:::\n\n\nAnd then count the number of hits, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt |>\n    group_by(subno) |>\n    count(type) |> \n  pivot_wider(names_from = type, values_from = n) \n```\n:::\n\n\nWe will need the following two functions later on. The first replaces all instances of `NA` with `0`; i.e. if there is a count of zero, then we have the value `NA` in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreplace_NA <- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n```\n:::\n\n\nThe second function provides a minor correction in case we have hit or false alarm rates of either `0` or `1`. Since a _rate_ $r$ is a relative frequency, which we interpret as a probability, it must lie within the range `0:1`: $0 < r < 1$. The function adds or subtracts a small number, depending on whether the rate is $0$ or $1$. In this case, neither function is necessary; we apply them anyway, for demonstration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrect_zero_one <- function(rate, e = 0.001) {\n    if (identical(rate, 0)) {\n        rate = rate + e\n    } else if (identical(rate, 1)) {\n        rate = rate - e\n    }\n    rate\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 5\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss\n   <fct> <int> <int> <int> <int>\n 1 53       33    20    25    22\n 2 54       39    14    28    19\n 3 55       36    17    31    16\n 4 56       43    10    38     9\n 5 57       35    18    29    18\n 6 58       41    12    30    17\n 7 59       46     7    21    26\n 8 60       38    15    33    14\n 9 61       42    11    25    22\n10 62       45     8    22    25\n# ℹ 21 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt_summary |>\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))\n\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 5\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss\n   <fct> <int> <int> <int> <int>\n 1 53       33    20    25    22\n 2 54       39    14    28    19\n 3 55       36    17    31    16\n 4 56       43    10    38     9\n 5 57       35    18    29    18\n 6 58       41    12    30    17\n 7 59       46     7    21    26\n 8 60       38    15    33    14\n 9 61       42    11    25    22\n10 62       45     8    22    25\n# ℹ 21 more rows\n```\n:::\n:::\n\n\nNext, we **estimate** the hit and false alarm rates, based on the observed number of hits and false alarms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt_summary |>\n    mutate(hit_rate = Hit/(Hit + Miss),\n           fa_rate = FA/(FA + CR))\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 7\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss hit_rate fa_rate\n   <fct> <int> <int> <int> <int>    <dbl>   <dbl>\n 1 53       33    20    25    22    0.532   0.377\n 2 54       39    14    28    19    0.596   0.264\n 3 55       36    17    31    16    0.660   0.321\n 4 56       43    10    38     9    0.809   0.189\n 5 57       35    18    29    18    0.617   0.340\n 6 58       41    12    30    17    0.638   0.226\n 7 59       46     7    21    26    0.447   0.132\n 8 60       38    15    33    14    0.702   0.283\n 9 61       42    11    25    22    0.532   0.208\n10 62       45     8    22    25    0.468   0.151\n# ℹ 21 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt_summary |>\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 7\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss hit_rate fa_rate\n   <fct> <int> <int> <int> <int>    <dbl>   <dbl>\n 1 53       33    20    25    22    0.532   0.377\n 2 54       39    14    28    19    0.596   0.264\n 3 55       36    17    31    16    0.660   0.321\n 4 56       43    10    38     9    0.809   0.189\n 5 57       35    18    29    18    0.617   0.340\n 6 58       41    12    30    17    0.638   0.226\n 7 59       46     7    21    26    0.447   0.132\n 8 60       38    15    33    14    0.702   0.283\n 9 61       42    11    25    22    0.532   0.208\n10 62       45     8    22    25    0.468   0.151\n# ℹ 21 more rows\n```\n:::\n:::\n\n\nGiven the hit and false alarm rates, we can calculate the value on the _latent strength_ variable that must result in the hit and false alarm rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt_summary |> \n  mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 9\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss hit_rate fa_rate     zhr    zfa\n   <fct> <int> <int> <int> <int>    <dbl>   <dbl>   <dbl>  <dbl>\n 1 53       33    20    25    22    0.532   0.377  0.0801 -0.312\n 2 54       39    14    28    19    0.596   0.264  0.242  -0.631\n 3 55       36    17    31    16    0.660   0.321  0.411  -0.466\n 4 56       43    10    38     9    0.809   0.189  0.872  -0.883\n 5 57       35    18    29    18    0.617   0.340  0.298  -0.413\n 6 58       41    12    30    17    0.638   0.226  0.354  -0.751\n 7 59       46     7    21    26    0.447   0.132 -0.134  -1.12 \n 8 60       38    15    33    14    0.702   0.283  0.531  -0.574\n 9 61       42    11    25    22    0.532   0.208  0.0801 -0.815\n10 62       45     8    22    25    0.468   0.151 -0.0801 -1.03 \n# ℹ 21 more rows\n```\n:::\n:::\n\n\nFinally, we compute $d'$, $k$ and $c$ using the formulae given above. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary <- sdt_summary |> \n  mutate(dprime = zhr - zfa,\n         k = -zfa,\n         c = -0.5 * (zhr + zfa)) |>\n    mutate(across(c(dprime, k, c), round, 2))\n\nsdt_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 31 × 12\n# Groups:   subno [31]\n   subno    CR    FA   Hit  Miss hit_rate fa_rate     zhr    zfa dprime     k\n   <fct> <int> <int> <int> <int>    <dbl>   <dbl>   <dbl>  <dbl>  <dbl> <dbl>\n 1 53       33    20    25    22    0.532   0.377  0.0801 -0.312   0.39  0.31\n 2 54       39    14    28    19    0.596   0.264  0.242  -0.631   0.87  0.63\n 3 55       36    17    31    16    0.660   0.321  0.411  -0.466   0.88  0.47\n 4 56       43    10    38     9    0.809   0.189  0.872  -0.883   1.76  0.88\n 5 57       35    18    29    18    0.617   0.340  0.298  -0.413   0.71  0.41\n 6 58       41    12    30    17    0.638   0.226  0.354  -0.751   1.1   0.75\n 7 59       46     7    21    26    0.447   0.132 -0.134  -1.12    0.98  1.12\n 8 60       38    15    33    14    0.702   0.283  0.531  -0.574   1.1   0.57\n 9 61       42    11    25    22    0.532   0.208  0.0801 -0.815   0.9   0.81\n10 62       45     8    22    25    0.468   0.151 -0.0801 -1.03    0.95  1.03\n# ℹ 21 more rows\n# ℹ 1 more variable: c <dbl>\n```\n:::\n:::\n\n\n# Memory experiment: single subject\n\nFor simplicity, we first look at the data from subject `53` only:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdt_summary |> \n  filter(subno == 53) |> \n  select(subno, hit_rate, fa_rate, zhr, zfa, dprime, k, c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n# Groups:   subno [1]\n  subno hit_rate fa_rate    zhr    zfa dprime     k     c\n  <fct>    <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1 53       0.532   0.377 0.0801 -0.312   0.39  0.31  0.12\n```\n:::\n:::\n",
    "supporting": [
      "signal-detection-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}