---
title: "Bayes Factors: Bayesianische Hypothesentests"
description: |
  Eine Alternative zu Null Hypothesis Significance Testing (NHST).
date: "2022-05-15"
author:
  - name: Andrew Ellis
    url: https://github.com/awellis
    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universit√§t Bern 
    affiliation-url: https://www.kog.psy.unibe.ch
    orcid: 0000-0002-2788-936X
license: CC BY
citation: true
bibliography: ../../bibliography.bib
format:
    html:
        toc: true
        code-link: true
execute: 
  cache: false
code-annotations: select
---

```{r}
#| include: false
# Set working directory of R
knitr::opts_knit$set(root.dir = "../../data/session-11")

library(webexercises)
```

:::{.callout-tip}
## Daten herunterladen

üëâ [`ExamQuestions.csv`](../../downloadable_files/ExamQuestions.csv)

üëâ [`SmartDrug.csv`](../../downloadable_files/SmartDrug.csv)
:::



:::{.callout-tip collapse="false"}
## Lernziele

In der heutigen Sitzung:

- Modellvergleiche mit Bayes Factors
- Bayes Factors mit Jasp berechnen
:::


:::{.callout-tip}
## Weiterf√ºhrende Literatur
- [Bayesian Inference in JASP: A Guide for Students](http://static.jasp-stats.org/Manuals/Bayesian_Guide_v0_12_2_1.pdf): Einf√ºhrung in die Bayesianische Statistik mit JASP (mit [Datens√§tzen](https://osf.io/8qtu2/)).

- Ein sehr gutes Lehrbuch Statistik mit JASP: [Learn Statistics with JASP](https://learnstatswithjasp.com/). Enth√§lt ein Kapitel zur Bayesianischen Statistik.

- Keysers, C., Gazzola, V., & Wagenmakers, E.-J. (2020). Using Bayes factor hypothesis testing in neuroscience to establish evidence of absence. Nature Neuroscience, 23(7), [https://doi.org/10.1038/s41593-020-0660-4](https://www.nature.com/articles/s41593-020-0660-4)
:::



## Absence of Evidence vs. Evidence of Absence

In den Neurowissenschaftler*innen ist es wichtig zu wissen, welche experimentellen Manipulationen einen Effekt haben. Genauso wichtig ist es jedoch zu wissen, welche Manipulationen _keinen Effekt_ haben. Diese Frage zu beantworten ist jedoch mit traditionellen statistischen Ans√§tzen schwierig.  Nicht signifikante Ergebnisse sind schwer zu interpretieren: Unterst√ºtzen sie die Nullhypothese oder sind sie einfach nicht informativ? 

:::{.callout-important}
## p-Werte
p-Werte sind [schwierig](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/) zu erkl√§ren. √úberlegen Sie sich nochmals, was man mit einem p-Wert genau aussagen kann. 
:::

Als Beispiel dient folgendes Experiment aus dem Paper von @keysersUsingBayesFactor2020. Nehmen wir an, dass der vordere cingul√§re Kortex (ACC) bei Ratten f√ºr die "emotionale Ansteckung" entscheidend ist und dass eine Deaktivierung des ACC durch lokale Injektion von Muscimol die emotionale Ansteckung im Vergleich zur Injektion von Kochsalzl√∂sung verringern sollte.

Ein injiziertes Tier beobachtete, wie ein Artgenosse Elektroschocks erhielt, und die Erstarrung wurde als Mass f√ºr die emotionale Ansteckung gemessen. Es gab auch eine nicht-soziale Kontrollbedingung, bei der das injizierte Tier einem schock-konditionierten Ton ausgesetzt wurde. In einem solchen Experiment ist es wichtig zu zeigen, dass die Manipulation (Injektion von Muscimol) die emotionale Ansteckung reduziert, aber nicht die Erstarrung im Allgemeinen. Dies bedeutet, dass in der Kontrollbedingung keinen Unterschied zwischen den Injektionen von Muscimol und Kochsalzl√∂sung geben sollte. 


In diesem Kapitel lernen wir nun eine alternative Methode kennen, um Evidenz zu quantifizieren: **Bayes Factors**. Alternativ zu p-Werten bieten Bayes Factors Evidenz f√ºr oder gegen Hypothesen. Bayes Factors sind ein kontinuierliches Mass, welches im Prinzip angibt, um wieviel wir unsere √úberzeugung f√ºr eine Hypothese √§ndern sollten, nachdem wir die Daten gesehen haben. 




## Bayesianische Statistik?
Wir wir in der letzten Sitzungen geh√∂rt haben, ist es wichtig, zwischen Parametersch√§tzung und Hypothesentest zu unterscheiden. Parametersch√§tzung bezeichnet den Prozess, einen oder mehrere Parameter in einem Modell zu sch√§tzen. Um diese Sch√§tzung durchzuf√ºhren, m√ºssen wir eine a-priori-Verteilung der Parameterwerte angeben. Hypothesentesten ist einen Vergleich zwischen mehreren Modellen, welche sich in ihren a-priori-Verteilungen unterscheiden.

Zum Beispiel hatten wir im Kartenspiel verschiedene Vorannahmen √ºber die F√§higkeiten der Spieler A und B. Eine Vorannahme dr√ºckte z.B. unsere √úberzeugung aus, dass beide Spieler gleich gut waren, w√§hrend eine andere Vorannahme die √úberzeugung ausdr√ºckte, dass entweder Spieler A oder Spieler B besser war. Diese Vorannahmen, zusammen mit unseren Annahmen √ºber die Verteilung der Daten, bilden ein Modell $\mathcal{M}$.

### Bayesianische Parametersch√§tzung

In der Bayesianischen Parametersch√§tzung konzentrieren wir uns auf ein Modell $\mathcal{M}$. Das Ziel unserer Inferenz ist die a-posteriori-Verteilung der Parameter $\theta$, und wir k√∂nnen diese durch Anwendung von Bayes' Theorem erhalten (unter Verwendung von Markov Chain Monte Carlo Sampling oder anderen Methoden).

$$ 
p(\theta | y) =  p(\theta) \cdot \frac{p(y | \theta)}{p(y)}
$$


Wir k√∂nnen die Bayes'sche Regel umformulieren, so dass die Abh√§ngigkeit der Parameter $\mathbf{\theta}$ vom Modell $\mathcal{M}$ eindeutig wird:
$$ 
p(\theta | y, \mathcal{M}) = \frac{p(y|\theta, \mathcal{M}) \cdot p(\theta | \mathcal{M})} {p(y | \mathcal{M})}
$$

wo $\mathcal{M}$ auf ein spezifisches Modell verweist. Dieses Modell wird durch die a-priori-Verteilung der Parameter $p(\theta | \mathcal{M})$ und die Wahrscheinlichkeit der Daten $p(y|\theta, \mathcal{M})$ bestimmt.

:::{.callout-important}
## Wahrscheinlichkeit der Daten
Die bedingte Wahrscheinlichkeit $p(y | \mathcal{M})$ gibt nun die Wahrscheinlichkeit der Daten an, gemittelt √ºber alle m√∂glichen Parameterwerte unter der Vorverteilung im Modell $\mathcal{M}$. Sie auch **Modell-Evidenz** genannt.
:::

Bei der Parametersch√§tzung spielt  $p(y | \mathcal{M})$ nur die Rolle eines Normalisierungsfaktors. Beim Modellvergleich ist $p(y | \mathcal{M})$ jedoch von zentraler Bedeutung.


Die Randwahrscheinlichkeit $p(y | \mathcal{M})$ ist der Nenner aus der Bayes'schen Formel:

$$ 
p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}
$$



und ist gegeben durch:
$$ 
p(y | \mathcal{M}) = \int{p(y | \theta, \mathcal{M}) p(\theta|\mathcal{M})d\theta}
$$


Dies bedeutet, dass wir die Wahrscheinlichkeit der Daten $p(y | \mathcal{M})$ erhalten, indem wir die Wahrscheinlichkeit der Daten f√ºr jeden m√∂glichen Parameterwert $\theta$ berechnen und dann √ºber alle m√∂glichen Parameterwerte mitteln.


Die a-priori-Verteilungen f√ºr $\theta$ sind wichtig, da sie die Wahrscheinlichkeit m√∂glicher Werte von $\theta$ bestimmen.





### Bayesianische Modellvergleiche

Nun wollen wir zwei verschiedene Modelle miteinander vergleichen -- wir wollen wissen, welches Modell die Daten besser erkl√§rt. Wir k√∂nnen die Bayes'sche Regel verwenden, um die Wahrscheinlichkeit der Modelle  $\mathcal{M1}$ und $\mathcal{M2}$ zu berechnen (gemittelt √ºber alle m√∂glichen Parameterwerte innerhalb des Modells):

$$ 
p(\mathcal{M}_1 | y) = \frac{P(y | \mathcal{M}_1) p(\mathcal{M}_1)}{p(y)} 
$$

und

$$ 
p(\mathcal{M}_2 | y) = \frac{P(y | \mathcal{M}_2) p(\mathcal{M}_2)}{p(y)} 
$$


Eine M√∂glichkeit w√§re, das Verh√§ltnis der beiden Wahrscheinlichkeiten zu berechnen: $p(\mathcal{M}_1 | y) / p(\mathcal{M}_2 | y)$. Dieses Verh√§ltnis wird als **posterior odds** bezeichnet:

$$ 
\frac{p(\mathcal{M}_1 | y) = \frac{P(y | \mathcal{M}_1) p(\mathcal{M}_1)}{p(y)}} {p(\mathcal{M}_2 | y) = \frac{P(y | \mathcal{M}_2) p(\mathcal{M}_2)}{p(y)}}
$$


$p(y)$ k√∂nnen wir k√ºrzen, da es in oberhalb und unterhalb des Bruchstriches im Nenner vorkommt. Wir erhalten:

$$ 
\frac{p(\mathcal{M}_1 | y) = P(y | \mathcal{M}_1) p(\mathcal{M}_1)} {p(\mathcal{M}_2 | y) = P(y | \mathcal{M}_2) p(\mathcal{M}_2)}
$$


Auf der linken Seite haben wir das Verh√§ltnis der a-posteriori Wahrscheinlichkeiten der beiden Modelle. Auf der rechten Seite haben wir das Verh√§ltnis der **Marginal Likelihoods** der beiden Modelle, multipliziert mit den a-priori Wahrscheinlichkeiten jedes Modells. Die Marginal Likelihoods (auch bekannt als Modell-Evidenz) zeigen, wie gut jedes Modell die Daten erkl√§rt. 

:::{.callout-tip}
## Marginal Likelihoods
Diese sagt uns im Prinzip, wie wahrscheinlich die Daten sind, wenn wir alle m√∂glichen Parameterwerte ber√ºcksichtigen. Die Marginal Likelihoods sind also die Wahrscheinlichkeit der Daten, gemittelt √ºber alle m√∂glichen Parameterwerte.
:::


$$
\underbrace{\frac{p(\mathcal{M}_1 | y)} {p(\mathcal{M}_2 | y)}}_\text{Posterior odds} = \underbrace{\frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}}_\text{Ratio of marginal likelihoods} \cdot \underbrace{ \frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)}}_\text{Prior odds}
$$



$\frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)}$ sind nun die **Prior Odds**, und $\frac{p(\mathcal{M}_1 | y)}{p(\mathcal{M}_2 | y)}$ sind die  **Posterior Odds**. Diese sagen uns, welches Modell wir a-priori und a-posteriori f√ºr wahrscheinlicher halten. Da unsere a-priori √úberzeugungen aber subjektiv sein k√∂nnen, sind wir eigentlich nur an dem Verh√§ltnis der marginalen Likelihoods interessiert. Wir k√∂nnen annehmen, dass a-priori die beiden Modelle gleichwahrscheinlich sind; das heisst, wir setzen die Prior Odds auf 1 setzen. So erhalten wir


$$
\frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}
$$

Dies ist der **Bayes Factor**. Falls $P(y | \mathcal{M}_1)$ gr√∂sser ist als $P(y | \mathcal{M}_2)$, dann ist der Bayes Factor gr√∂sser als 1. Falls $P(y | \mathcal{M}_1)$ kleiner ist als $P(y | \mathcal{M}_2)$, dann ist der Bayes Factor kleiner als 1. Der **Bayes Factor** gibt also direkt an, welches Modell die Daten besser erkl√§rt.

Wenn wir zwei Modelle $\mathcal{M}_1$ und $\mathcal{M}_2$ vergleichen, wird der **Bayes Factor** oftmals so geschrieben:

$$ BF_{12} = \frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}$$


$BF_{12}$ ist also der Bayes Factor f√ºr $\mathcal{M}_1$ und gibt an um wieviel $\mathcal{M}_1$ die Daten besser "erkl√§rt".


Als Beispiel, wenn wir ein $BF_{12} = 5$ erhalten, bedeutet dies, dass die Daten 5 Mal wahrscheinlicher unter Modell 1 als unter Modell 2 aufgetreten sind. Umgekehrt, wenn $BF_{12} = 0.2$, dann sind die Daten 5 Mal wahrscheinlicher unter Modell 2 aufgetreten.

Wenn wir $BF_{12} = 0.2$ erhalten, ist es einfacher, Z√§hler und Nenner zu vertauschen:

$$ BF_{21} = \frac{P(y | \mathcal{M}_2)}{P(y | \mathcal{M}_1)}$$


Die folgenden [Interpretationen](https://www.statology.org/bayes-factor/) von Bayes Factors werden manchmal verwendet, obwohl es nicht wirklich notwendig ist, diese zu klassifizieren. Bayes Factors sind ein kontinuierliches Mass f√ºr Evidenz.



## Hypothesentesten

Wir f√ºhren oft Modellvergleiche zwischen einer Nullhypothese $\mathcal{H}_0$ und einer alternativen Hypothese $\mathcal{H}_1$ durch (Die Begriffe "Modell" und "Hypothese" werden synonym verwendet). Eine Nullhypothese bedeutet, dass wir den Wert des Parameters auf einen bestimmten Wert festlegen, z.B. $\theta = 0.5$. Die alternative Hypothese bedeutet, dass wir den Wert des Parameters nicht festlegen, sondern eine a-priori Verteilung annehmen.

:::{.callout-important}
## Alternativhypothese
Im Gegensatz zu NHST muss die Alternativhypothese spezifiziert werden. Mit anderen Worten, die Parameter m√ºssen eine a-priori Verteilung erhalten.
:::


In JASP werden Bayes Factors (BF) so berichtet:

$$ BF_{10} = \frac{P(y | \mathcal{H}_1)}{P(y | \mathcal{H}_0)}$$

Dies ist ein BF f√ºr eine ungerichtete Alternative $\mathcal{H}_1$ gegen die Nullhypothese $\mathcal{H}_0$. Wenn wir einen gerichteten Test durchf√ºhren, dann wird der BF entweder so ($>0$):

$$ BF_{+0} = \frac{P(y | \mathcal{H}_+)}{P(y | \mathcal{H}_0)}$$

oder so ($<0$) berichtet.
$$ BF_{-0} = \frac{P(y | \mathcal{H}_-)}{P(y | \mathcal{H}_0)}$$


Wenn wir nun einen BF f√ºr die Nullhypothese wollen, k√∂nnen wir einfach den Kehrwert von $BF_{10}$ nehmen:

$$ BF_{01} = \frac{1}{BF_{10}}$$


:::{.callout-tip collapse="true"}
## Savage-Dickey Density Ratio

Wenn wir zwei genestete Modelle vergleichen, d.h. ein Modell mit 1 freiem Parameter und ein Nullmodell (in dem dieser Parameter auf einen bestimmten Wert festgelegt ist), k√∂nnen wir das Savage-Dickey Density Ratio verwenden, um den Bayes-Faktor zu berechnen.

- Nullmodell ($\mathcal{H}_0$ or $\mathcal{M}_0$): $\theta = \theta_0$

- Alternativmodell ($\mathcal{H}_1$ or $\mathcal{M}_1$): $\theta \neq \theta_0$

$\theta$ braucht unter $\mathcal{H}_1$ eine a-priori Verteilung. Im Kartenspiel-Beispiel vom letzten Kapitel war dies eine Beta-Verteilung mit $\alpha = 1$ und $\beta = 1$: $\theta \sim \text{Beta}(1, 1)$. Diese Hypothese besagt, dass alle Parameterwerte gleich wahrscheinlich sind.

Das Savage-Dickey Density Ratio ist eine verinfachte Methode, um einen Bayes Factor zu erhalten - wir m√ºssen einfach $\mathcal{M}_1$ betrachten, und die a-posterior Verteilung durch die a-prior Verteilung an der Stelle $\theta_0$ teilen.

Als Beispiel: stellen Sie sich vor, dass eine Person 9 von 10 Ja/Nein Fragen korrekt beantwortet.

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
```

```{r}
#| eval: false
#| include: true
library(tidyverse)
```


```{r}
n_correct <- 9
n_questions <- 10
```

Wir wollen wissen: wie wahrscheinlich ist es, dass die Person geraten hat ($\theta=0.5$)?

Wir nehmen nun eine uniforme a-priori Verteilung f√ºr  $\theta$ an:
$$
p(\theta) = Beta(1, 1)
$$


```{r}
pd <- tibble(
    x = seq(0, 1, by = .01),
    Prior = dbeta(x, 1, 1)
)
ggplot(pd, aes(x, Prior)) +
    geom_line(linewidth = 1.5) +
    coord_cartesian(xlim = 0:1, ylim = c(0, 6), expand = 0.01) +
    labs(y = "Density", x = bquote(theta))
```

Im n√§chsten Schritt wenden wir Bayes Theorem an, um die a-posteriori Verteilung zu erhalten:

$$
p(\theta|y) = Beta(1 + 9, 1 + 1)
$$

Unser "Test-Wert" ist $Œ∏ = 0.5$. Wir k√∂nnen nun sowohl die a-priori als auch die a-posteriori Verteilung an dieser Stelle auswerten:

Prior:

```{r}
(dprior <- dbeta(0.5, 1, 1))
```

Posterior:

```{r}
(dposterior <- dbeta(0.5, 10, 2))
```

Der Bayes Factor f√ºr die Nullhypothese $BF_{01}$ ist die Wahrscheinlichkeit der a-posteriori Verteilung geteilt durch die Wahrscheinlichkeit der a-priori Verteilung an der Stelle $\theta_0$:

```{r}
BF01 <- dposterior / dprior
```

```{r}
BF01
```

Da eine Zahl $<0$ nicht so leicht interpretierbar ist, berechnen wir $BF_{10}$:

```{r}
(BF10 <- 1/BF01)
```

Die Daten (9 von 10 Fragen richtig beantwortet) sind also $9.3$ mal wahrscheinlicher unter der Alternativhypothese als unter der Nullhypothese.

In JASP wird der BF folgendermassen grafisch dargestellt. Wir plotten sowohl die a-priori als auch die a-posteriori Verteilung, und die Wahrscheinlichkeitsdichten an der Stelle $\theta_0 = 0.5$.

Die Wahrscheinlichkeitsdichte von $\theta_0$ wird kleiner, nachdem wir die Daten ber√ºcksichtigt haben. Anders formuliert: Die Wahrscheinlichkeit von $\theta_0$ hat abgenommen, nachdem wir die Daten beobachtet haben. Dies erm√∂glicht uns zu schlussfolgern, dass die Daten (9 von 10) etwa 9 Mal wahrscheinlicher unter $\mathcal{M_1}$ sind.

```{r}
pd <- pd |> 
    mutate(Posterior = dbeta(x, 1 + n_correct, 1 + (n_questions-n_correct)))

pdw <- pd |> 
  pivot_longer(names_to = "Type", 
               values_to = "density", 
               Prior:Posterior)
pdw |> 
  ggplot(aes(x, density, col = Type)) +
  geom_vline(xintercept = 0.5, linetype = "dotted") +
  geom_line(linewidth = 1.5) +
  scale_x_continuous(expand = expansion(0.01)) +
  scale_color_viridis_d(end = 0.8) +
  labs(y = "Density", x = bquote(theta)) +
  annotate("point", x = c(.5, .5), 
           y = c(pdw$density[pdw$x == .5]),
           size = 4) +
  annotate("label",
    x = c(.5, .5),
    y = pdw$density[pdw$x == .5],
    label = round(pdw$density[pdw$x == .5], 3),
    vjust = -.5
  )
```



```{r}
filter(pd, x == .5) |>
  mutate(
    BF01 = Posterior / Prior,
    BF10 = 1 / BF01) 
```

:::


:::{.callout-caution}
## Hands-on

Eine Student*in beantwortet 9 von 10 Fragen korrekt.

- Was ist die Evidenz daf√ºr, dass die Student*in geraten hat (Gl√ºck hatte)?
- Was ist die Evidenz daf√ºr, dass die Wahrscheinlichkeit einer korrekten Antwort der Student*in gr√∂sser als $0.5$ ist?

Importieren Sie den Datensatz `ExamQuestions.csv` ins JASP und f√ºhren Sie einen __Bayesian binomial rate test__ durch.
:::


## Bayesian t-Test

Um einen Bayesianischen t-Test durchzuf√ºhren, m√ºssen wir ebenfalls Parameter sch√§tzen, und zwei Modelle miteinander vergleichen. Dies werden wir anhand zweier Beispiele in JASP demonstrieren.


:::{.callout-caution}
## Smart Drug

In diesem Beispiel m√∂chten wir die IQ-Werte von zwei Gruppen vergleichen. Einer der Gruppen wurde ein Medikament zur Steigerung der Intelligenz verabreicht, die andere ist die Kontrollgruppe.

Importieren Sie den Datensatz `SmartDrug.csv` ins JASP und f√ºhren Sie einen __Bayesian t-test__ durch.
:::


:::{.callout-caution}
## Horizontal Eye Movements
√ñffnen Sie den Datensatz `Eye Movements` in JASP: `Open > Data Library > T-Tests > Eye Movements`. In diesem Datensatz wird die Anzahl erinnerter W√∂rter zwischen zwei Gruppen verglichen. In der einen Gruppe wurden die Probanden instruiert, w√§hrend der Erinnerungsphase auf einen zentralen Punkt zu fixieren; in der anderen Gruppe wurden die Probanden instruiert, horizontale Sakkaden auszuf√ºhren.
:::
