---
title: "Bayes Factors: Bayesianische Hypothesentests"
description: |
  Eine Alternative zu Null Hypothesis Significance Testing (NHST).
date: "2022-05-15"
author:
  - name: Andrew Ellis
    url: https://github.com/awellis
    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, UniversitÃ¤t Bern 
    affiliation-url: https://www.kog.psy.unibe.ch
    orcid: 0000-0002-2788-936X
license: CC BY
citation: true
bibliography: ../../bibliography.bib
format:
    html:
        toc: true
        code-link: true
execute: 
  cache: false
code-annotations: select
---

```{r}
#| include: false
# Set working directory of R
knitr::opts_knit$set(root.dir = "../../data/session-11")

library(webexercises)
```

:::{.callout-tip}
## Daten herunterladen

ğŸ‘‰ [`ExamQuestions.csv`](../../downloadable_files/ExamQuestions.csv)

ğŸ‘‰ [`SmartDrug.csv`](../../downloadable_files/SmartDrug.csv)
:::



:::{.callout-tip collapse="false"}
## Lernziele

In der heutigen Sitzung:

- Modellvergleiche mit Bayes Factors
- Bayes Factors mit Jasp berechnen
:::


:::{.callout-tip}
## WeiterfÃ¼hrende Literatur
- [Bayesian Inference in JASP: A Guide for Students](http://static.jasp-stats.org/Manuals/Bayesian_Guide_v0_12_2_1.pdf): EinfÃ¼hrung in die Bayesianische Statistik mit JASP (mit [DatensÃ¤tzen](https://osf.io/8qtu2/)).

- Ein sehr gutes Lehrbuch Statistik mit JASP: [Learn Statistics with JASP](https://learnstatswithjasp.com/). EnthÃ¤lt ein Kapitel zur Bayesianischen Statistik.

- Keysers, C., Gazzola, V., & Wagenmakers, E.-J. (2020). Using Bayes factor hypothesis testing in neuroscience to establish evidence of absence. Nature Neuroscience, 23(7), [https://doi.org/10.1038/s41593-020-0660-4](https://www.nature.com/articles/s41593-020-0660-4)
:::



## Absence of Evidence vs. Evidence of Absence

In den Neurowissenschaftler*innen ist es wichtig zu wissen, welche experimentellen Manipulationen einen Effekt haben. Genauso wichtig ist es jedoch zu wissen, welche Manipulationen _keinen Effekt_ haben. Diese Frage zu beantworten ist jedoch mit traditionellen statistischen AnsÃ¤tzen schwierig.  Nicht signifikante Ergebnisse sind schwer zu interpretieren: UnterstÃ¼tzen sie die Nullhypothese oder sind sie einfach nicht informativ? 

:::{.callout-important}
## p-Werte
p-Werte sind [schwierig](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/) zu erklÃ¤ren. Ãœberlegen Sie sich nochmals, was man mit einem p-Wert genau aussagen kann. 
:::

Als Beispiel dient folgendes Experiment aus dem Paper von @keysersUsingBayesFactor2020. Nehmen wir an, dass der vordere cingulÃ¤re Kortex (ACC) bei Ratten fÃ¼r die "emotionale Ansteckung" entscheidend ist und dass eine Deaktivierung des ACC durch lokale Injektion von Muscimol die emotionale Ansteckung im Vergleich zur Injektion von KochsalzlÃ¶sung verringern sollte.

Ein injiziertes Tier beobachtete, wie ein Artgenosse Elektroschocks erhielt, und die Erstarrung wurde als Mass fÃ¼r die emotionale Ansteckung gemessen. Es gab auch eine nicht-soziale Kontrollbedingung, bei der das injizierte Tier einem schock-konditionierten Ton ausgesetzt wurde. In einem solchen Experiment ist es wichtig zu zeigen, dass die Manipulation (Injektion von Muscimol) die emotionale Ansteckung reduziert, aber nicht die Erstarrung im Allgemeinen. Dies bedeutet, dass in der Kontrollbedingung keinen Unterschied zwischen den Injektionen von Muscimol und KochsalzlÃ¶sung geben sollte. 


In diesem Kapitel lernen wir nun eine alternative Methode kennen, um Evidenz zu quantifizieren: **Bayes Factors**. Alternativ zu p-Werten bieten Bayes Factors Evidenz fÃ¼r oder gegen Hypothesen. Bayes Factors sind ein kontinuierliches Mass, welches im Prinzip angibt, um wieviel wir unsere Ãœberzeugung fÃ¼r eine Hypothese Ã¤ndern sollten, nachdem wir die Daten gesehen haben. 




## Bayesianische Statistik?
Wir wir in der letzten Sitzungen gehÃ¶rt haben, ist es wichtig, zwischen ParameterschÃ¤tzung und Hypothesentest zu unterscheiden. ParameterschÃ¤tzung bezeichnet den Prozess, einen oder mehrere Parameter in einem Modell zu schÃ¤tzen. Um diese SchÃ¤tzung durchzufÃ¼hren, mÃ¼ssen wir eine a-priori-Verteilung der Parameterwerte angeben. Hypothesentesten ist einen Vergleich zwischen mehreren Modellen, welche sich in ihren a-priori-Verteilungen unterscheiden.

Zum Beispiel hatten wir im Kartenspiel verschiedene Vorannahmen Ã¼ber die FÃ¤higkeiten der Spieler A und B. Eine Vorannahme drÃ¼ckte z.B. unsere Ãœberzeugung aus, dass beide Spieler gleich gut waren, wÃ¤hrend eine andere Vorannahme die Ãœberzeugung ausdrÃ¼ckte, dass entweder Spieler A oder Spieler B besser war. Diese Vorannahmen, zusammen mit unseren Annahmen Ã¼ber die Verteilung der Daten, bilden ein Modell $\mathcal{M}$.

### Bayesianische ParameterschÃ¤tzung

In der Bayesianischen ParameterschÃ¤tzung konzentrieren wir uns auf ein Modell $\mathcal{M}$. Das Ziel unserer Inferenz ist die a-posteriori-Verteilung der Parameter $\theta$, und wir kÃ¶nnen diese durch Anwendung von Bayes' Theorem erhalten (unter Verwendung von Markov Chain Monte Carlo Sampling oder anderen Methoden).

$$ 
p(\theta | y) =  p(\theta) \cdot \frac{p(y | \theta)}{p(y)}
$$


Wir kÃ¶nnen die Bayes'sche Regel umformulieren, so dass die AbhÃ¤ngigkeit der Parameter $\mathbf{\theta}$ vom Modell $\mathcal{M}$ eindeutig wird:
$$ 
p(\theta | y, \mathcal{M}) = \frac{p(y|\theta, \mathcal{M}) \cdot p(\theta | \mathcal{M})} {p(y | \mathcal{M})}
$$

wo $\mathcal{M}$ auf ein spezifisches Modell verweist. Dieses Modell wird durch die a-priori-Verteilung der Parameter $p(\theta | \mathcal{M})$ und die Wahrscheinlichkeit der Daten $p(y|\theta, \mathcal{M})$ bestimmt.

:::{.callout-important}
## Wahrscheinlichkeit der Daten
Die bedingte Wahrscheinlichkeit $p(y | \mathcal{M})$ gibt nun die Wahrscheinlichkeit der Daten an, gemittelt Ã¼ber alle mÃ¶glichen Parameterwerte unter der Vorverteilung im Modell $\mathcal{M}$. Sie auch **Modell-Evidenz** genannt.
:::

Bei der ParameterschÃ¤tzung spielt  $p(y | \mathcal{M})$ nur die Rolle eines Normalisierungsfaktors. Beim Modellvergleich ist $p(y | \mathcal{M})$ jedoch von zentraler Bedeutung.


Die Randwahrscheinlichkeit $p(y | \mathcal{M})$ ist der Nenner aus der Bayes'schen Formel:

$$ 
p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}
$$



und ist gegeben durch:
$$ 
p(y | \mathcal{M}) = \int{p(y | \theta, \mathcal{M}) p(\theta|\mathcal{M})d\theta}
$$


Dies bedeutet, dass wir die Wahrscheinlichkeit der Daten $p(y | \mathcal{M})$ erhalten, indem wir die Wahrscheinlichkeit der Daten fÃ¼r jeden mÃ¶glichen Parameterwert $\theta$ berechnen und dann Ã¼ber alle mÃ¶glichen Parameterwerte mitteln.


Die a-priori-Verteilungen fÃ¼r $\theta$ sind wichtig, da sie die Wahrscheinlichkeit mÃ¶glicher Werte von $\theta$ bestimmen.





### Bayesianische Modellvergleiche

Nun wollen wir zwei verschiedene Modelle miteinander vergleichen -- wir wollen wissen, welches Modell die Daten besser erklÃ¤rt. Wir kÃ¶nnen die Bayes'sche Regel verwenden, um die Wahrscheinlichkeit der Modelle  $\mathcal{M1}$ und $\mathcal{M2}$ zu berechnen (gemittelt Ã¼ber alle mÃ¶glichen Parameterwerte innerhalb des Modells):

$$ 
p(\mathcal{M}_1 | y) = \frac{P(y | \mathcal{M}_1) p(\mathcal{M}_1)}{p(y)} 
$$

und

$$ 
p(\mathcal{M}_2 | y) = \frac{P(y | \mathcal{M}_2) p(\mathcal{M}_2)}{p(y)} 
$$


Eine MÃ¶glichkeit wÃ¤re, das VerhÃ¤ltnis der beiden Wahrscheinlichkeiten zu berechnen: $p(\mathcal{M}_1 | y) / p(\mathcal{M}_2 | y)$. Dieses VerhÃ¤ltnis wird als **posterior odds** bezeichnet:

$$ 
\frac{p(\mathcal{M}_1 | y) = \frac{P(y | \mathcal{M}_1) p(\mathcal{M}_1)}{p(y)}} {p(\mathcal{M}_2 | y) = \frac{P(y | \mathcal{M}_2) p(\mathcal{M}_2)}{p(y)}}
$$


$p(y)$ kÃ¶nnen wir kÃ¼rzen, da es in oberhalb und unterhalb des Bruchstriches im Nenner vorkommt. Wir erhalten:

$$ 
\frac{p(\mathcal{M}_1 | y) = P(y | \mathcal{M}_1) p(\mathcal{M}_1)} {p(\mathcal{M}_2 | y) = P(y | \mathcal{M}_2) p(\mathcal{M}_2)}
$$


Auf der linken Seite haben wir das VerhÃ¤ltnis der a-posteriori Wahrscheinlichkeiten der beiden Modelle. Auf der rechten Seite haben wir das VerhÃ¤ltnis der **Marginal Likelihoods** der beiden Modelle, multipliziert mit den a-priori Wahrscheinlichkeiten jedes Modells. Die Marginal Likelihoods (auch bekannt als Modell-Evidenz) zeigen, wie gut jedes Modell die Daten erklÃ¤rt. 

:::{.callout-tip}
## Marginal Likelihoods
Diese geben darÃ¼ber Auskunft, wie wahrscheinlich die Daten sind, wenn wir alle mÃ¶glichen Parameterwerte berÃ¼cksichtigen. Die Marginal Likelihoods sind also die Wahrscheinlichkeit der Daten, gemittelt Ã¼ber alle mÃ¶glichen Parameterwerte.
:::


$$
\underbrace{\frac{p(\mathcal{M}_1 | y)} {p(\mathcal{M}_2 | y)}}_\text{Posterior odds} = \underbrace{\frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}}_\text{Ratio of marginal likelihoods} \cdot \underbrace{ \frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)}}_\text{Prior odds}
$$



$\frac{p(\mathcal{M}_1)}{p(\mathcal{M}_2)}$ sind nun die **Prior Odds**, und $\frac{p(\mathcal{M}_1 | y)}{p(\mathcal{M}_2 | y)}$ sind die  **Posterior Odds**. Diese sagen uns, welches Modell wir a-priori und a-posteriori fÃ¼r wahrscheinlicher halten. Da unsere a-priori Ãœberzeugungen aber subjektiv sein kÃ¶nnen, sind wir eigentlich nur an dem VerhÃ¤ltnis der marginalen Likelihoods interessiert. Wir kÃ¶nnen annehmen, dass a-priori die beiden Modelle gleichwahrscheinlich sind; das heisst, wir setzen die Prior Odds auf 1 setzen. So erhalten wir


$$
\frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}
$$

Dies ist der **Bayes Factor**. Falls $P(y | \mathcal{M}_1)$ grÃ¶sser ist als $P(y | \mathcal{M}_2)$, dann ist der Bayes Factor grÃ¶sser als 1. Falls $P(y | \mathcal{M}_1)$ kleiner ist als $P(y | \mathcal{M}_2)$, dann ist der Bayes Factor kleiner als 1. Der **Bayes Factor** gibt also direkt an, welches Modell die Daten besser erklÃ¤rt.

Wenn wir zwei Modelle $\mathcal{M}_1$ und $\mathcal{M}_2$ vergleichen, wird der **Bayes Factor** oftmals so geschrieben:

$$ BF_{12} = \frac{P(y | \mathcal{M}_1)}{P(y | \mathcal{M}_2)}$$


$BF_{12}$ ist also der Bayes Factor fÃ¼r $\mathcal{M}_1$ und gibt an um wieviel $\mathcal{M}_1$ die Daten besser "erklÃ¤rt".


Als Beispiel, wenn wir ein $BF_{12} = 5$ erhalten, bedeutet dies, dass die Daten 5 Mal wahrscheinlicher unter Modell 1 als unter Modell 2 aufgetreten sind. Umgekehrt, wenn $BF_{12} = 0.2$, dann sind die Daten 5 Mal wahrscheinlicher unter Modell 2 aufgetreten.

Wenn wir $BF_{12} = 0.2$ erhalten, ist es einfacher, ZÃ¤hler und Nenner zu vertauschen:

$$ BF_{21} = \frac{P(y | \mathcal{M}_2)}{P(y | \mathcal{M}_1)}$$


Die folgenden [Interpretationen](https://www.statology.org/bayes-factor/) von Bayes Factors werden manchmal verwendet, obwohl es nicht wirklich notwendig ist, diese zu klassifizieren. Bayes Factors sind ein kontinuierliches Mass fÃ¼r Evidenz.



## Hypothesentesten

Wir fÃ¼hren oft Modellvergleiche zwischen einer Nullhypothese $\mathcal{H}_0$ und einer alternativen Hypothese $\mathcal{H}_1$ durch (Die Begriffe "Modell" und "Hypothese" werden synonym verwendet). Eine Nullhypothese bedeutet, dass wir den Wert des Parameters auf einen bestimmten Wert festlegen, z.B. $\theta = 0.5$. Die alternative Hypothese bedeutet, dass wir den Wert des Parameters nicht festlegen, sondern eine a-priori Verteilung annehmen.

:::{.callout-important}
## Alternativhypothese
Im Gegensatz zu NHST muss die Alternativhypothese spezifiziert werden. Mit anderen Worten, die Parameter mÃ¼ssen eine a-priori Verteilung erhalten.
:::


In JASP werden Bayes Factors (BF) so berichtet:

$$ BF_{10} = \frac{P(y | \mathcal{H}_1)}{P(y | \mathcal{H}_0)}$$

Dies ist ein BF fÃ¼r eine ungerichtete Alternative $\mathcal{H}_1$ gegen die Nullhypothese $\mathcal{H}_0$. Wenn wir einen gerichteten Test durchfÃ¼hren, dann wird der BF entweder so ($>0$):

$$ BF_{+0} = \frac{P(y | \mathcal{H}_+)}{P(y | \mathcal{H}_0)}$$

oder so ($<0$) berichtet.
$$ BF_{-0} = \frac{P(y | \mathcal{H}_-)}{P(y | \mathcal{H}_0)}$$


Wenn wir nun einen BF fÃ¼r die Nullhypothese wollen, kÃ¶nnen wir einfach den Kehrwert von $BF_{10}$ nehmen:

$$ BF_{01} = \frac{1}{BF_{10}}$$


:::{.callout-tip collapse="true"}
## Savage-Dickey Density Ratio

Wenn wir zwei genestete Modelle vergleichen, d.h. ein Modell mit 1 freiem Parameter und ein Nullmodell (in dem dieser Parameter auf einen bestimmten Wert festgelegt ist), kÃ¶nnen wir das Savage-Dickey Density Ratio verwenden, um den Bayes-Faktor zu berechnen.

- Nullmodell ($\mathcal{H}_0$ or $\mathcal{M}_0$): $\theta = \theta_0$

- Alternativmodell ($\mathcal{H}_1$ or $\mathcal{M}_1$): $\theta \neq \theta_0$

$\theta$ braucht unter $\mathcal{H}_1$ eine a-priori Verteilung. Im Kartenspiel-Beispiel vom letzten Kapitel war dies eine Beta-Verteilung mit $\alpha = 1$ und $\beta = 1$: $\theta \sim \text{Beta}(1, 1)$. Diese Hypothese besagt, dass alle Parameterwerte gleich wahrscheinlich sind.

Das Savage-Dickey Density Ratio ist eine verinfachte Methode, um einen Bayes Factor zu erhalten - wir mÃ¼ssen einfach $\mathcal{M}_1$ betrachten, und die a-posterior Verteilung durch die a-prior Verteilung an der Stelle $\theta_0$ teilen.

Als Beispiel: stellen Sie sich vor, dass eine Person 9 von 10 Ja/Nein Fragen korrekt beantwortet.

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
```

```{r}
#| eval: false
#| include: true
library(tidyverse)
```


```{r}
n_correct <- 9
n_questions <- 10
```

Wir wollen wissen: wie wahrscheinlich ist es, dass die Person geraten hat ($\theta=0.5$)?

Wir nehmen nun eine uniforme a-priori Verteilung fÃ¼r  $\theta$ an:
$$
p(\theta) = Beta(1, 1)
$$


```{r}
pd <- tibble(
    x = seq(0, 1, by = .01),
    Prior = dbeta(x, 1, 1)
)
ggplot(pd, aes(x, Prior)) +
    geom_line(linewidth = 1.5) +
    coord_cartesian(xlim = 0:1, ylim = c(0, 6), expand = 0.01) +
    labs(y = "Density", x = bquote(theta))
```

Im nÃ¤chsten Schritt wenden wir Bayes Theorem an, um die a-posteriori Verteilung zu erhalten:

$$
p(\theta|y) = Beta(1 + 9, 1 + 1)
$$

Unser "Test-Wert" ist $Î¸ = 0.5$. Wir kÃ¶nnen nun sowohl die a-priori als auch die a-posteriori Verteilung an dieser Stelle auswerten:

Prior:

```{r}
(dprior <- dbeta(0.5, 1, 1))
```

Posterior:

```{r}
(dposterior <- dbeta(0.5, 10, 2))
```

Der Bayes Factor fÃ¼r die Nullhypothese $BF_{01}$ ist die Wahrscheinlichkeit der a-posteriori Verteilung geteilt durch die Wahrscheinlichkeit der a-priori Verteilung an der Stelle $\theta_0$:

```{r}
BF01 <- dposterior / dprior
```

```{r}
BF01
```

Da eine Zahl $<0$ nicht so leicht interpretierbar ist, berechnen wir $BF_{10}$:

```{r}
(BF10 <- 1/BF01)
```

Die Daten (9 von 10 Fragen richtig beantwortet) sind also $9.3$ mal wahrscheinlicher unter der Alternativhypothese als unter der Nullhypothese.

In JASP wird der BF folgendermassen grafisch dargestellt. Wir plotten sowohl die a-priori als auch die a-posteriori Verteilung, und die Wahrscheinlichkeitsdichten an der Stelle $\theta_0 = 0.5$.

Die Wahrscheinlichkeitsdichte von $\theta_0$ wird kleiner, nachdem wir die Daten berÃ¼cksichtigt haben. Anders formuliert: Die Wahrscheinlichkeit von $\theta_0$ hat abgenommen, nachdem wir die Daten beobachtet haben. Dies ermÃ¶glicht uns zu schlussfolgern, dass die Daten (9 von 10) etwa 9 Mal wahrscheinlicher unter $\mathcal{M_1}$ sind.

```{r}
pd <- pd |> 
    mutate(Posterior = dbeta(x, 1 + n_correct, 1 + (n_questions-n_correct)))

pdw <- pd |> 
  pivot_longer(names_to = "Type", 
               values_to = "density", 
               Prior:Posterior)
pdw |> 
  ggplot(aes(x, density, col = Type)) +
  geom_vline(xintercept = 0.5, linetype = "dotted") +
  geom_line(linewidth = 1.5) +
  scale_x_continuous(expand = expansion(0.01)) +
  scale_color_viridis_d(end = 0.8) +
  labs(y = "Density", x = bquote(theta)) +
  annotate("point", x = c(.5, .5), 
           y = c(pdw$density[pdw$x == .5]),
           size = 4) +
  annotate("label",
    x = c(.5, .5),
    y = pdw$density[pdw$x == .5],
    label = round(pdw$density[pdw$x == .5], 3),
    vjust = -.5
  )
```



```{r}
filter(pd, x == .5) |>
  mutate(
    BF01 = Posterior / Prior,
    BF10 = 1 / BF01) 
```

:::


:::{.callout-caution}
## Hands-on

Eine Student*in beantwortet 9 von 10 Fragen korrekt.

- Was ist die Evidenz dafÃ¼r, dass die Student*in geraten hat (GlÃ¼ck hatte)?
- Was ist die Evidenz dafÃ¼r, dass die Wahrscheinlichkeit einer korrekten Antwort der Student*in grÃ¶sser als $0.5$ ist?

Importieren Sie den Datensatz `ExamQuestions.csv` ins JASP und fÃ¼hren Sie einen __Bayesian binomial rate test__ durch.
:::


## Bayesian t-Test

Um einen Bayesianischen t-Test durchzufÃ¼hren, mÃ¼ssen wir ebenfalls Parameter schÃ¤tzen, und zwei Modelle miteinander vergleichen. Dies werden wir anhand zweier Beispiele in JASP demonstrieren.


:::{.callout-caution}
## Smart Drug

In diesem Beispiel mÃ¶chten wir die IQ-Werte von zwei Gruppen vergleichen. Einer der Gruppen wurde ein Medikament zur Steigerung der Intelligenz verabreicht, die andere ist die Kontrollgruppe.

Importieren Sie den Datensatz `SmartDrug.csv` ins JASP und fÃ¼hren Sie einen __Bayesian t-test__ durch.
:::


:::{.callout-caution}
## Horizontal Eye Movements
Ã–ffnen Sie den Datensatz `Eye Movements` in JASP: `Open > Data Library > T-Tests > Eye Movements`. In diesem Datensatz wird die Anzahl erinnerter WÃ¶rter zwischen zwei Gruppen verglichen. In der einen Gruppe wurden die Probanden instruiert, wÃ¤hrend der Erinnerungsphase auf einen zentralen Punkt zu fixieren; in der anderen Gruppe wurden die Probanden instruiert, horizontale Sakkaden auszufÃ¼hren.
:::

Eine sehr gute interaktive Visualisierung eines Bayesian `two-sample t test` finden Sie unter folgendem Link: [ğŸ‘‰ rpsychologist.com/d3/bayes](https://rpsychologist.com/d3/bayes/).
